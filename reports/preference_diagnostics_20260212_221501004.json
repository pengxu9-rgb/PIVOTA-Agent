{
  "ok": true,
  "run_id": "20260212_221501004",
  "final_verdict": "SHIP_VARIANT1",
  "executive": {
    "overall": {
      "slice_key": "overall",
      "n_votes": 6,
      "sample_count": 4,
      "baseline_votes": 1,
      "variant1_votes": 2,
      "tie_votes": 2,
      "cannot_tell_votes": 1,
      "head_to_head_votes": 3,
      "baseline_win_rate": 0.333,
      "variant1_win_rate": 0.667,
      "tie_rate": 0.333,
      "cannot_tell_rate": 0.167,
      "disagreement_rate": 0.667,
      "baseline_wilson_low": 0.061,
      "baseline_wilson_high": 0.792,
      "variant1_wilson_low": 0.208,
      "variant1_wilson_high": 0.939,
      "_sample_ids": [
        "s1",
        "s2",
        "s3",
        "s4"
      ]
    },
    "iaa": {
      "overall_simple_agreement": 0.5,
      "overall_kappa": 0.333,
      "overlap_samples_total": 2,
      "overlap_samples_labeled_by_2plus": 2,
      "sufficient": true
    }
  },
  "overlay_consistency_gate": {
    "pass": true,
    "thresholds": {
      "coverage_min": 0.98,
      "consistency_min": 0.98,
      "epsilon": 0.000001
    },
    "counts": {
      "eval_rows_total": 4,
      "overlay_present_rows": 4,
      "consistent_rows": 4,
      "joinable_rows": 4,
      "issue_rows": 0
    },
    "rates": {
      "coverage_rate": 1,
      "consistency_rate": 1,
      "joinable_consistency_rate": 1,
      "missing_rate": 0
    },
    "top_issues": []
  },
  "slices": {
    "by_source": [
      {
        "slice_key": "internal",
        "n_votes": 3,
        "sample_count": 2,
        "baseline_votes": 1,
        "variant1_votes": 2,
        "tie_votes": 0,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 3,
        "baseline_win_rate": 0.333,
        "variant1_win_rate": 0.667,
        "tie_rate": 0,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.333,
        "baseline_wilson_low": 0.061,
        "baseline_wilson_high": 0.792,
        "variant1_wilson_low": 0.208,
        "variant1_wilson_high": 0.939
      },
      {
        "slice_key": "lapa",
        "n_votes": 2,
        "sample_count": 1,
        "baseline_votes": 0,
        "variant1_votes": 0,
        "tie_votes": 2,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 0,
        "baseline_win_rate": null,
        "variant1_win_rate": null,
        "tie_rate": 1,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0,
        "baseline_wilson_low": null,
        "baseline_wilson_high": null,
        "variant1_wilson_low": null,
        "variant1_wilson_high": null
      },
      {
        "slice_key": "celebamaskhq",
        "n_votes": 1,
        "sample_count": 1,
        "baseline_votes": 0,
        "variant1_votes": 0,
        "tie_votes": 0,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 0,
        "baseline_win_rate": null,
        "variant1_win_rate": null,
        "tie_rate": 0,
        "cannot_tell_rate": 1,
        "disagreement_rate": 0,
        "baseline_wilson_low": null,
        "baseline_wilson_high": null,
        "variant1_wilson_low": null,
        "variant1_wilson_high": null
      }
    ],
    "by_module": [
      {
        "slice_key": "forehead",
        "n_votes": 6,
        "sample_count": 4,
        "baseline_votes": 1,
        "variant1_votes": 2,
        "tie_votes": 2,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 3,
        "baseline_win_rate": 0.333,
        "variant1_win_rate": 0.667,
        "tie_rate": 0.333,
        "cannot_tell_rate": 0.167,
        "disagreement_rate": 0.667,
        "baseline_wilson_low": 0.061,
        "baseline_wilson_high": 0.792,
        "variant1_wilson_low": 0.208,
        "variant1_wilson_high": 0.939
      },
      {
        "slice_key": "nose",
        "n_votes": 6,
        "sample_count": 4,
        "baseline_votes": 1,
        "variant1_votes": 2,
        "tie_votes": 2,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 3,
        "baseline_win_rate": 0.333,
        "variant1_win_rate": 0.667,
        "tie_rate": 0.333,
        "cannot_tell_rate": 0.167,
        "disagreement_rate": 0.667,
        "baseline_wilson_low": 0.061,
        "baseline_wilson_high": 0.792,
        "variant1_wilson_low": 0.208,
        "variant1_wilson_high": 0.939
      }
    ],
    "by_risk_hair": [
      {
        "slice_key": "mid(0.10-0.25)",
        "n_votes": 4,
        "sample_count": 3,
        "baseline_votes": 1,
        "variant1_votes": 2,
        "tie_votes": 0,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 3,
        "baseline_win_rate": 0.333,
        "variant1_win_rate": 0.667,
        "tie_rate": 0,
        "cannot_tell_rate": 0.25,
        "disagreement_rate": 0.5,
        "baseline_wilson_low": 0.061,
        "baseline_wilson_high": 0.792,
        "variant1_wilson_low": 0.208,
        "variant1_wilson_high": 0.939
      },
      {
        "slice_key": "low(<0.10)",
        "n_votes": 2,
        "sample_count": 1,
        "baseline_votes": 0,
        "variant1_votes": 0,
        "tie_votes": 2,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 0,
        "baseline_win_rate": null,
        "variant1_win_rate": null,
        "tie_rate": 1,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0,
        "baseline_wilson_low": null,
        "baseline_wilson_high": null,
        "variant1_wilson_low": null,
        "variant1_wilson_high": null
      }
    ],
    "by_risk_leakage": [
      {
        "slice_key": "low(<0.05)",
        "n_votes": 5,
        "sample_count": 3,
        "baseline_votes": 1,
        "variant1_votes": 2,
        "tie_votes": 2,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 3,
        "baseline_win_rate": 0.333,
        "variant1_win_rate": 0.667,
        "tie_rate": 0.4,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.6,
        "baseline_wilson_low": 0.061,
        "baseline_wilson_high": 0.792,
        "variant1_wilson_low": 0.208,
        "variant1_wilson_high": 0.939
      },
      {
        "slice_key": "mid(0.05-0.15)",
        "n_votes": 1,
        "sample_count": 1,
        "baseline_votes": 0,
        "variant1_votes": 0,
        "tie_votes": 0,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 0,
        "baseline_win_rate": null,
        "variant1_win_rate": null,
        "tie_rate": 0,
        "cannot_tell_rate": 1,
        "disagreement_rate": 0,
        "baseline_wilson_low": null,
        "baseline_wilson_high": null,
        "variant1_wilson_low": null,
        "variant1_wilson_high": null
      }
    ],
    "by_risk_min_pixels": [
      {
        "slice_key": "mid(49-128)",
        "n_votes": 3,
        "sample_count": 2,
        "baseline_votes": 0,
        "variant1_votes": 1,
        "tie_votes": 2,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 1,
        "baseline_win_rate": 0,
        "variant1_win_rate": 1,
        "tie_rate": 0.667,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.333,
        "baseline_wilson_low": 0,
        "baseline_wilson_high": 0.793,
        "variant1_wilson_low": 0.207,
        "variant1_wilson_high": 1
      },
      {
        "slice_key": "tiny(<=16)",
        "n_votes": 2,
        "sample_count": 1,
        "baseline_votes": 1,
        "variant1_votes": 1,
        "tie_votes": 0,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 2,
        "baseline_win_rate": 0.5,
        "variant1_win_rate": 0.5,
        "tie_rate": 0,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.5,
        "baseline_wilson_low": 0.095,
        "baseline_wilson_high": 0.905,
        "variant1_wilson_low": 0.095,
        "variant1_wilson_high": 0.905
      },
      {
        "slice_key": "small(17-48)",
        "n_votes": 1,
        "sample_count": 1,
        "baseline_votes": 0,
        "variant1_votes": 0,
        "tie_votes": 0,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 0,
        "baseline_win_rate": null,
        "variant1_win_rate": null,
        "tie_rate": 0,
        "cannot_tell_rate": 1,
        "disagreement_rate": 0,
        "baseline_wilson_low": null,
        "baseline_wilson_high": null,
        "variant1_wilson_low": null,
        "variant1_wilson_high": null
      }
    ],
    "by_guard": [
      {
        "slice_key": "no",
        "n_votes": 5,
        "sample_count": 3,
        "baseline_votes": 1,
        "variant1_votes": 2,
        "tie_votes": 2,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 3,
        "baseline_win_rate": 0.333,
        "variant1_win_rate": 0.667,
        "tie_rate": 0.4,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.6,
        "baseline_wilson_low": 0.061,
        "baseline_wilson_high": 0.792,
        "variant1_wilson_low": 0.208,
        "variant1_wilson_high": 0.939
      },
      {
        "slice_key": "yes",
        "n_votes": 1,
        "sample_count": 1,
        "baseline_votes": 0,
        "variant1_votes": 0,
        "tie_votes": 0,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 0,
        "baseline_win_rate": null,
        "variant1_win_rate": null,
        "tie_rate": 0,
        "cannot_tell_rate": 1,
        "disagreement_rate": 0,
        "baseline_wilson_low": null,
        "baseline_wilson_high": null,
        "variant1_wilson_low": null,
        "variant1_wilson_high": null
      }
    ],
    "by_overlay_diff": [
      {
        "slice_key": "high(>=0.03)",
        "n_votes": 2,
        "sample_count": 2,
        "baseline_votes": 0,
        "variant1_votes": 1,
        "tie_votes": 0,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 1,
        "baseline_win_rate": 0,
        "variant1_win_rate": 1,
        "tie_rate": 0,
        "cannot_tell_rate": 0.5,
        "disagreement_rate": 0.5,
        "baseline_wilson_low": 0,
        "baseline_wilson_high": 0.793,
        "variant1_wilson_low": 0.207,
        "variant1_wilson_high": 1
      },
      {
        "slice_key": "mid(0.01-0.03)",
        "n_votes": 2,
        "sample_count": 1,
        "baseline_votes": 0,
        "variant1_votes": 0,
        "tie_votes": 2,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 0,
        "baseline_win_rate": null,
        "variant1_win_rate": null,
        "tie_rate": 1,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0,
        "baseline_wilson_low": null,
        "baseline_wilson_high": null,
        "variant1_wilson_low": null,
        "variant1_wilson_high": null
      },
      {
        "slice_key": "very_low(<0.01)",
        "n_votes": 2,
        "sample_count": 1,
        "baseline_votes": 1,
        "variant1_votes": 1,
        "tie_votes": 0,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 2,
        "baseline_win_rate": 0.5,
        "variant1_win_rate": 0.5,
        "tie_rate": 0,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.5,
        "baseline_wilson_low": 0.095,
        "baseline_wilson_high": 0.905,
        "variant1_wilson_low": 0.095,
        "variant1_wilson_high": 0.905
      }
    ],
    "by_confidence": [
      {
        "slice_key": "high(>=4)",
        "n_votes": 4,
        "sample_count": 3,
        "baseline_votes": 0,
        "variant1_votes": 1,
        "tie_votes": 2,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 1,
        "baseline_win_rate": 0,
        "variant1_win_rate": 1,
        "tie_rate": 0.5,
        "cannot_tell_rate": 0.25,
        "disagreement_rate": 0.5,
        "baseline_wilson_low": 0,
        "baseline_wilson_high": 0.793,
        "variant1_wilson_low": 0.207,
        "variant1_wilson_high": 1
      },
      {
        "slice_key": "low(<=2)",
        "n_votes": 2,
        "sample_count": 1,
        "baseline_votes": 1,
        "variant1_votes": 1,
        "tie_votes": 0,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 2,
        "baseline_win_rate": 0.5,
        "variant1_win_rate": 0.5,
        "tie_rate": 0,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.5,
        "baseline_wilson_low": 0.095,
        "baseline_wilson_high": 0.905,
        "variant1_wilson_low": 0.095,
        "variant1_wilson_high": 0.905
      }
    ]
  },
  "disagreement_drivers": [
    {
      "slice_type": "module",
      "slice_key": "forehead",
      "n_votes": 6,
      "cannot_tell_rate": 0.167,
      "disagreement_rate": 0.667,
      "avg_split_close": 0.45,
      "avg_hair_overlap": 0.138,
      "avg_leakage_bg": 0.045,
      "avg_min_module_pixels": 57,
      "avg_overlay_diff_ratio": 0.056,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.409
    },
    {
      "slice_type": "module",
      "slice_key": "nose",
      "n_votes": 6,
      "cannot_tell_rate": 0.167,
      "disagreement_rate": 0.667,
      "avg_split_close": 0.45,
      "avg_hair_overlap": 0.138,
      "avg_leakage_bg": 0.045,
      "avg_min_module_pixels": 57,
      "avg_overlay_diff_ratio": 0.056,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.409
    },
    {
      "slice_type": "hair_bucket",
      "slice_key": "mid(0.10-0.25)",
      "n_votes": 4,
      "cannot_tell_rate": 0.25,
      "disagreement_rate": 0.5,
      "avg_split_close": 0.5,
      "avg_hair_overlap": 0.167,
      "avg_leakage_bg": 0.047,
      "avg_min_module_pixels": 54.667,
      "avg_overlay_diff_ratio": 0.068,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.388
    },
    {
      "slice_type": "confidence",
      "slice_key": "high(>=4)",
      "n_votes": 4,
      "cannot_tell_rate": 0.25,
      "disagreement_rate": 0.5,
      "avg_split_close": 0.333,
      "avg_hair_overlap": 0.117,
      "avg_leakage_bg": 0.05,
      "avg_min_module_pixels": 70.667,
      "avg_overlay_diff_ratio": 0.073,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.362
    },
    {
      "slice_type": "guard_triggered",
      "slice_key": "no",
      "n_votes": 5,
      "cannot_tell_rate": 0,
      "disagreement_rate": 0.6,
      "avg_split_close": 0.433,
      "avg_hair_overlap": 0.123,
      "avg_leakage_bg": 0.03,
      "avg_min_module_pixels": 66.667,
      "avg_overlay_diff_ratio": 0.048,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.305
    },
    {
      "slice_type": "leakage_bucket",
      "slice_key": "low(<0.05)",
      "n_votes": 5,
      "cannot_tell_rate": 0,
      "disagreement_rate": 0.6,
      "avg_split_close": 0.433,
      "avg_hair_overlap": 0.123,
      "avg_leakage_bg": 0.03,
      "avg_min_module_pixels": 66.667,
      "avg_overlay_diff_ratio": 0.048,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.305
    },
    {
      "slice_type": "source",
      "slice_key": "internal",
      "n_votes": 3,
      "cannot_tell_rate": 0,
      "disagreement_rate": 0.333,
      "avg_split_close": 0.5,
      "avg_hair_overlap": 0.16,
      "avg_leakage_bg": 0.025,
      "avg_min_module_pixels": 68,
      "avg_overlay_diff_ratio": 0.063,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.208
    },
    {
      "slice_type": "min_pixels_bucket",
      "slice_key": "mid(49-128)",
      "n_votes": 3,
      "cannot_tell_rate": 0,
      "disagreement_rate": 0.333,
      "avg_split_close": 0.25,
      "avg_hair_overlap": 0.085,
      "avg_leakage_bg": 0.03,
      "avg_min_module_pixels": 92,
      "avg_overlay_diff_ratio": 0.07,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.171
    }
  ],
  "actions": [
    {
      "rank": 1,
      "title": "Increase visual separability in A/B overlays",
      "what_to_change": "Update `scripts/preference_round1_real_runbook.mjs` overlay rendering to add contour-diff inset and run a focused sweep with `PREFERENCE_MAX_EDGE=768`.",
      "target_slice": "hair_bucket:mid(0.10-0.25)",
      "why": "cannot_tell_rate=0.25 with avg_split_close=0.5 indicates small visible deltas.",
      "validate": "Run `make preference-round1-real-pack ... TARGET_TOTAL=80 PREFERENCE_MAX_EDGE=768` then `make preference-final ...`; expect cannot_tell_rate to drop by >=0.05 without lowering IAA."
    },
    {
      "rank": 2,
      "title": "Tighten labeling rubric for ambiguous modules",
      "what_to_change": "Refine `label_studio/project_preference_ab.xml` instructions and `docs/GOLD_LABELING_GUIDE.md` for tie/cannot_tell usage on under-eye and low-detail regions.",
      "target_slice": "module:under_eye_*",
      "why": "under-eye slice cannot_tell_rate=- and disagreement_rate=-.",
      "validate": "Re-run overlap subset (>=40) and check IAA improves (kappa +0.05) while cannot_tell on under-eye decreases."
    },
    {
      "rank": 3,
      "title": "Harden forehead hair/skin boundary behavior",
      "what_to_change": "Keep hair-aware forehead clip path and prioritize forehead/hair hard cases for skinmask+hair-mask retraining decision; tune oval clip params in offline AB (`DIAG_FACE_OVAL_CLIP_MIN_PIXELS`, `DIAG_FACE_OVAL_CLIP_MIN_KEEP_RATIO`).",
      "target_slice": "forehead + high hair_overlap_est",
      "why": "forehead variant win=0.667, baseline win=0.333; disagreement driver highlights boundary instability.",
      "validate": "Run `make eval-gold-ab ...` + preference rerun on hair-high subset; expect forehead variant win +>=0.10 and lower disagreement in high-hair bucket."
    },
    {
      "rank": 4,
      "title": "Stabilize hard-case guard strategy",
      "what_to_change": "Tune guard behavior on hard samples via offline variant sweep (variant2 under-eye guard relaxation + fallback behavior) and compare only guard-triggered subset.",
      "target_slice": "module:forehead",
      "why": "high disagreement slice score=0.409 suggests unstable model outputs on hard cases.",
      "validate": "Run a guard-triggered mini-pack (`TARGET_TOTAL=80`, stress-heavy) and expect disagreement_rate drop by >=0.08 with no increase in cannot_tell."
    },
    {
      "rank": 5,
      "title": "Address internal style mismatch and pipeline artifacts",
      "what_to_change": "If internal remains worse, add internal-style slices to training/eval packs and inspect crop pipeline (`input_thumb` generation) for over-aggressive resizing.",
      "target_slice": "source:internal",
      "why": "internal disagreement_rate=0.333 vs overall=0.667; crossset leakage=-.",
      "validate": "Run internal-only preference round (`LIMIT_INTERNAL` up, external down) and confirm internal cannot_tell/disagreement converge toward external slices."
    }
  ],
  "contentious_export": {
    "path": "artifacts/preference_contentious_20260212_221501004.jsonl",
    "samples": 4
  },
  "optional_context": {
    "crossset": null,
    "gold_eval": null
  },
  "proposer_summary": {
    "suggested_overlay_diff_filter_min": 0.01,
    "very_low_overlay_diff_vote_rate": 0.333,
    "guidance": "prioritize contentious samples with overlay_diff_ratio>=0.01 and downweight overlay_diff_ratio<0.01"
  },
  "generated_at": "2026-02-12T23:37:29.209Z",
  "inputs": {
    "manifest": "artifacts/_smoke_overlay_gate_20260212_221501004/manifest.json",
    "eval": "artifacts/_smoke_overlay_gate_20260212_221501004/eval.jsonl",
    "labels": "artifacts/_smoke_overlay_gate_20260212_221501004/labels.ndjson",
    "crossset": null,
    "gold_eval_md": null
  },
  "artifacts": {
    "diagnostics_md": "reports/preference_diagnostics_20260212_221501004.md",
    "diagnostics_json": "reports/preference_diagnostics_20260212_221501004.json",
    "contentious_jsonl": "artifacts/preference_contentious_20260212_221501004.jsonl"
  }
}
