{
  "ok": true,
  "run_id": "20260212_200001001",
  "final_verdict": "SHIP_VARIANT1",
  "executive": {
    "overall": {
      "slice_key": "overall",
      "n_votes": 26,
      "sample_count": 20,
      "baseline_votes": 6,
      "variant1_votes": 14,
      "tie_votes": 3,
      "cannot_tell_votes": 3,
      "head_to_head_votes": 20,
      "baseline_win_rate": 0.3,
      "variant1_win_rate": 0.7,
      "tie_rate": 0.115,
      "cannot_tell_rate": 0.115,
      "disagreement_rate": 0.462,
      "baseline_wilson_low": 0.145,
      "baseline_wilson_high": 0.519,
      "variant1_wilson_low": 0.481,
      "variant1_wilson_high": 0.855,
      "_sample_ids": [
        "41d7d6a4bfe8949413b9",
        "678c2af642e3d58f271a",
        "7b226ea45d70d7aa0e77",
        "85ca93eecf3b21be0662",
        "a213e09b7b8a33adc6e9",
        "ab317a1473400cffad2b",
        "c5ab4e42a32fdf0c2ed7",
        "celebamaskhq_idx_1",
        "celebamaskhq_idx_2",
        "celebamaskhq_idx_3",
        "celebamaskhq_idx_4",
        "celebamaskhq_idx_5",
        "celebamaskhq_idx_6",
        "lapa_idx_1",
        "lapa_idx_2",
        "lapa_idx_3",
        "lapa_idx_4",
        "lapa_idx_5",
        "lapa_idx_6",
        "lapa_idx_7"
      ]
    },
    "iaa": {
      "overall_simple_agreement": 0.667,
      "overall_kappa": 0.455,
      "overlap_samples_total": 6,
      "overlap_samples_labeled_by_2plus": 6,
      "sufficient": true
    }
  },
  "slices": {
    "by_source": [
      {
        "slice_key": "internal",
        "n_votes": 10,
        "sample_count": 7,
        "baseline_votes": 4,
        "variant1_votes": 4,
        "tie_votes": 2,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 8,
        "baseline_win_rate": 0.5,
        "variant1_win_rate": 0.5,
        "tie_rate": 0.2,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.6,
        "baseline_wilson_low": 0.215,
        "baseline_wilson_high": 0.785,
        "variant1_wilson_low": 0.215,
        "variant1_wilson_high": 0.785
      },
      {
        "slice_key": "lapa",
        "n_votes": 10,
        "sample_count": 7,
        "baseline_votes": 1,
        "variant1_votes": 6,
        "tie_votes": 1,
        "cannot_tell_votes": 2,
        "head_to_head_votes": 7,
        "baseline_win_rate": 0.143,
        "variant1_win_rate": 0.857,
        "tie_rate": 0.1,
        "cannot_tell_rate": 0.2,
        "disagreement_rate": 0.4,
        "baseline_wilson_low": 0.026,
        "baseline_wilson_high": 0.513,
        "variant1_wilson_low": 0.487,
        "variant1_wilson_high": 0.974
      },
      {
        "slice_key": "celebamaskhq",
        "n_votes": 6,
        "sample_count": 6,
        "baseline_votes": 1,
        "variant1_votes": 4,
        "tie_votes": 0,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 5,
        "baseline_win_rate": 0.2,
        "variant1_win_rate": 0.8,
        "tie_rate": 0,
        "cannot_tell_rate": 0.167,
        "disagreement_rate": 0.333,
        "baseline_wilson_low": 0.036,
        "baseline_wilson_high": 0.624,
        "variant1_wilson_low": 0.376,
        "variant1_wilson_high": 0.964
      }
    ],
    "by_module": [
      {
        "slice_key": "chin",
        "n_votes": 26,
        "sample_count": 20,
        "baseline_votes": 17,
        "variant1_votes": 9,
        "tie_votes": 0,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 26,
        "baseline_win_rate": 0.654,
        "variant1_win_rate": 0.346,
        "tie_rate": 0,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.346,
        "baseline_wilson_low": 0.462,
        "baseline_wilson_high": 0.806,
        "variant1_wilson_low": 0.194,
        "variant1_wilson_high": 0.538
      },
      {
        "slice_key": "forehead",
        "n_votes": 26,
        "sample_count": 20,
        "baseline_votes": 10,
        "variant1_votes": 16,
        "tie_votes": 0,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 26,
        "baseline_win_rate": 0.385,
        "variant1_win_rate": 0.615,
        "tie_rate": 0,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.385,
        "baseline_wilson_low": 0.224,
        "baseline_wilson_high": 0.575,
        "variant1_wilson_low": 0.425,
        "variant1_wilson_high": 0.776
      },
      {
        "slice_key": "left_cheek",
        "n_votes": 26,
        "sample_count": 20,
        "baseline_votes": 0,
        "variant1_votes": 0,
        "tie_votes": 26,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 0,
        "baseline_win_rate": null,
        "variant1_win_rate": null,
        "tie_rate": 1,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0,
        "baseline_wilson_low": null,
        "baseline_wilson_high": null,
        "variant1_wilson_low": null,
        "variant1_wilson_high": null
      },
      {
        "slice_key": "nose",
        "n_votes": 26,
        "sample_count": 20,
        "baseline_votes": 10,
        "variant1_votes": 16,
        "tie_votes": 0,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 26,
        "baseline_win_rate": 0.385,
        "variant1_win_rate": 0.615,
        "tie_rate": 0,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.385,
        "baseline_wilson_low": 0.224,
        "baseline_wilson_high": 0.575,
        "variant1_wilson_low": 0.425,
        "variant1_wilson_high": 0.776
      },
      {
        "slice_key": "right_cheek",
        "n_votes": 26,
        "sample_count": 20,
        "baseline_votes": 7,
        "variant1_votes": 6,
        "tie_votes": 0,
        "cannot_tell_votes": 13,
        "head_to_head_votes": 13,
        "baseline_win_rate": 0.538,
        "variant1_win_rate": 0.462,
        "tie_rate": 0,
        "cannot_tell_rate": 0.5,
        "disagreement_rate": 0.5,
        "baseline_wilson_low": 0.291,
        "baseline_wilson_high": 0.768,
        "variant1_wilson_low": 0.232,
        "variant1_wilson_high": 0.709
      }
    ],
    "by_risk_hair": [
      {
        "slice_key": "low(<0.10)",
        "n_votes": 26,
        "sample_count": 20,
        "baseline_votes": 6,
        "variant1_votes": 14,
        "tie_votes": 3,
        "cannot_tell_votes": 3,
        "head_to_head_votes": 20,
        "baseline_win_rate": 0.3,
        "variant1_win_rate": 0.7,
        "tie_rate": 0.115,
        "cannot_tell_rate": 0.115,
        "disagreement_rate": 0.462,
        "baseline_wilson_low": 0.145,
        "baseline_wilson_high": 0.519,
        "variant1_wilson_low": 0.481,
        "variant1_wilson_high": 0.855
      }
    ],
    "by_risk_leakage": [
      {
        "slice_key": "low(<0.05)",
        "n_votes": 26,
        "sample_count": 20,
        "baseline_votes": 6,
        "variant1_votes": 14,
        "tie_votes": 3,
        "cannot_tell_votes": 3,
        "head_to_head_votes": 20,
        "baseline_win_rate": 0.3,
        "variant1_win_rate": 0.7,
        "tie_rate": 0.115,
        "cannot_tell_rate": 0.115,
        "disagreement_rate": 0.462,
        "baseline_wilson_low": 0.145,
        "baseline_wilson_high": 0.519,
        "variant1_wilson_low": 0.481,
        "variant1_wilson_high": 0.855
      }
    ],
    "by_risk_min_pixels": [
      {
        "slice_key": "tiny(<=16)",
        "n_votes": 26,
        "sample_count": 20,
        "baseline_votes": 6,
        "variant1_votes": 14,
        "tie_votes": 3,
        "cannot_tell_votes": 3,
        "head_to_head_votes": 20,
        "baseline_win_rate": 0.3,
        "variant1_win_rate": 0.7,
        "tie_rate": 0.115,
        "cannot_tell_rate": 0.115,
        "disagreement_rate": 0.462,
        "baseline_wilson_low": 0.145,
        "baseline_wilson_high": 0.519,
        "variant1_wilson_low": 0.481,
        "variant1_wilson_high": 0.855
      }
    ],
    "by_guard": [
      {
        "slice_key": "no",
        "n_votes": 26,
        "sample_count": 20,
        "baseline_votes": 6,
        "variant1_votes": 14,
        "tie_votes": 3,
        "cannot_tell_votes": 3,
        "head_to_head_votes": 20,
        "baseline_win_rate": 0.3,
        "variant1_win_rate": 0.7,
        "tie_rate": 0.115,
        "cannot_tell_rate": 0.115,
        "disagreement_rate": 0.462,
        "baseline_wilson_low": 0.145,
        "baseline_wilson_high": 0.519,
        "variant1_wilson_low": 0.481,
        "variant1_wilson_high": 0.855
      }
    ],
    "by_overlay_diff": [
      {
        "slice_key": "very_low(<0.01)",
        "n_votes": 26,
        "sample_count": 20,
        "baseline_votes": 6,
        "variant1_votes": 14,
        "tie_votes": 3,
        "cannot_tell_votes": 3,
        "head_to_head_votes": 20,
        "baseline_win_rate": 0.3,
        "variant1_win_rate": 0.7,
        "tie_rate": 0.115,
        "cannot_tell_rate": 0.115,
        "disagreement_rate": 0.462,
        "baseline_wilson_low": 0.145,
        "baseline_wilson_high": 0.519,
        "variant1_wilson_low": 0.481,
        "variant1_wilson_high": 0.855
      }
    ],
    "by_confidence": [
      {
        "slice_key": "high(>=4)",
        "n_votes": 13,
        "sample_count": 13,
        "baseline_votes": 1,
        "variant1_votes": 6,
        "tie_votes": 3,
        "cannot_tell_votes": 3,
        "head_to_head_votes": 7,
        "baseline_win_rate": 0.143,
        "variant1_win_rate": 0.857,
        "tie_rate": 0.231,
        "cannot_tell_rate": 0.231,
        "disagreement_rate": 0.538,
        "baseline_wilson_low": 0.026,
        "baseline_wilson_high": 0.513,
        "variant1_wilson_low": 0.487,
        "variant1_wilson_high": 0.974
      },
      {
        "slice_key": "low(<=2)",
        "n_votes": 13,
        "sample_count": 13,
        "baseline_votes": 5,
        "variant1_votes": 8,
        "tie_votes": 0,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 13,
        "baseline_win_rate": 0.385,
        "variant1_win_rate": 0.615,
        "tie_rate": 0,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.385,
        "baseline_wilson_low": 0.177,
        "baseline_wilson_high": 0.645,
        "variant1_wilson_low": 0.355,
        "variant1_wilson_high": 0.823
      }
    ]
  },
  "disagreement_drivers": [
    {
      "slice_type": "module",
      "slice_key": "right_cheek",
      "n_votes": 26,
      "cannot_tell_rate": 0.5,
      "disagreement_rate": 0.5,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.425
    },
    {
      "slice_type": "confidence",
      "slice_key": "high(>=4)",
      "n_votes": 13,
      "cannot_tell_rate": 0.231,
      "disagreement_rate": 0.538,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.319
    },
    {
      "slice_type": "source",
      "slice_key": "lapa",
      "n_votes": 10,
      "cannot_tell_rate": 0.2,
      "disagreement_rate": 0.4,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.25
    },
    {
      "slice_type": "source",
      "slice_key": "internal",
      "n_votes": 10,
      "cannot_tell_rate": 0,
      "disagreement_rate": 0.6,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0,
      "likely_cause": "internal photo style mismatch",
      "issue_area": "model issue",
      "score": 0.24
    },
    {
      "slice_type": "guard_triggered",
      "slice_key": "no",
      "n_votes": 26,
      "cannot_tell_rate": 0.115,
      "disagreement_rate": 0.462,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.237
    },
    {
      "slice_type": "hair_bucket",
      "slice_key": "low(<0.10)",
      "n_votes": 26,
      "cannot_tell_rate": 0.115,
      "disagreement_rate": 0.462,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.237
    },
    {
      "slice_type": "leakage_bucket",
      "slice_key": "low(<0.05)",
      "n_votes": 26,
      "cannot_tell_rate": 0.115,
      "disagreement_rate": 0.462,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.237
    },
    {
      "slice_type": "min_pixels_bucket",
      "slice_key": "tiny(<=16)",
      "n_votes": 26,
      "cannot_tell_rate": 0.115,
      "disagreement_rate": 0.462,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.237
    },
    {
      "slice_type": "overlay_diff_bucket",
      "slice_key": "very_low(<0.01)",
      "n_votes": 26,
      "cannot_tell_rate": 0.115,
      "disagreement_rate": 0.462,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.237
    },
    {
      "slice_type": "source",
      "slice_key": "celebamaskhq",
      "n_votes": 6,
      "cannot_tell_rate": 0.167,
      "disagreement_rate": 0.333,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.208
    },
    {
      "slice_type": "confidence",
      "slice_key": "low(<=2)",
      "n_votes": 13,
      "cannot_tell_rate": 0,
      "disagreement_rate": 0.385,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.154
    },
    {
      "slice_type": "module",
      "slice_key": "forehead",
      "n_votes": 26,
      "cannot_tell_rate": 0,
      "disagreement_rate": 0.385,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.154
    }
  ],
  "actions": [
    {
      "rank": 1,
      "title": "Increase visual separability in A/B overlays",
      "what_to_change": "Update `scripts/preference_round1_real_runbook.mjs` overlay rendering to add contour-diff inset and run a focused sweep with `PREFERENCE_MAX_EDGE=768`.",
      "target_slice": "module:right_cheek",
      "why": "cannot_tell_rate=0.5 with avg_split_close=0 indicates small visible deltas.",
      "validate": "Run `make preference-round1-real-pack ... TARGET_TOTAL=80 PREFERENCE_MAX_EDGE=768` then `make preference-final ...`; expect cannot_tell_rate to drop by >=0.05 without lowering IAA."
    },
    {
      "rank": 2,
      "title": "Tighten labeling rubric for ambiguous modules",
      "what_to_change": "Refine `label_studio/project_preference_ab.xml` instructions and `docs/GOLD_LABELING_GUIDE.md` for tie/cannot_tell usage on under-eye and low-detail regions.",
      "target_slice": "module:under_eye_*",
      "why": "under-eye slice cannot_tell_rate=- and disagreement_rate=-.",
      "validate": "Re-run overlap subset (>=40) and check IAA improves (kappa +0.05) while cannot_tell on under-eye decreases."
    },
    {
      "rank": 3,
      "title": "Harden forehead hair/skin boundary behavior",
      "what_to_change": "Keep hair-aware forehead clip path and prioritize forehead/hair hard cases for skinmask+hair-mask retraining decision; tune oval clip params in offline AB (`DIAG_FACE_OVAL_CLIP_MIN_PIXELS`, `DIAG_FACE_OVAL_CLIP_MIN_KEEP_RATIO`).",
      "target_slice": "forehead + high hair_overlap_est",
      "why": "forehead variant win=0.615, baseline win=0.385; disagreement driver highlights boundary instability.",
      "validate": "Run `make eval-gold-ab ...` + preference rerun on hair-high subset; expect forehead variant win +>=0.10 and lower disagreement in high-hair bucket."
    },
    {
      "rank": 4,
      "title": "Stabilize hard-case guard strategy",
      "what_to_change": "Tune guard behavior on hard samples via offline variant sweep (variant2 under-eye guard relaxation + fallback behavior) and compare only guard-triggered subset.",
      "target_slice": "module:right_cheek",
      "why": "high disagreement slice score=0.425 suggests unstable model outputs on hard cases.",
      "validate": "Run a guard-triggered mini-pack (`TARGET_TOTAL=80`, stress-heavy) and expect disagreement_rate drop by >=0.08 with no increase in cannot_tell."
    },
    {
      "rank": 5,
      "title": "Address internal style mismatch and pipeline artifacts",
      "what_to_change": "If internal remains worse, add internal-style slices to training/eval packs and inspect crop pipeline (`input_thumb` generation) for over-aggressive resizing.",
      "target_slice": "source:internal",
      "why": "internal disagreement_rate=0.6 vs overall=0.462; crossset leakage=-.",
      "validate": "Run internal-only preference round (`LIMIT_INTERNAL` up, external down) and confirm internal cannot_tell/disagreement converge toward external slices."
    }
  ],
  "contentious_export": {
    "path": "artifacts/preference_contentious_20260212_200001001.jsonl",
    "samples": 20
  },
  "optional_context": {
    "crossset": null,
    "gold_eval": null
  },
  "proposer_summary": {
    "suggested_overlay_diff_filter_min": 0.01,
    "very_low_overlay_diff_vote_rate": 1,
    "guidance": "prioritize contentious samples with overlay_diff_ratio>=0.01 and downweight overlay_diff_ratio<0.01"
  },
  "generated_at": "2026-02-12T16:16:17.065Z",
  "inputs": {
    "manifest": "artifacts/preference_round1_20260212_200001001/manifest.json",
    "eval": "reports/eval_preference_20260212_200001001.jsonl",
    "labels": "artifacts/preference_round1_20260212_200001001/preference_labels.ndjson",
    "crossset": null,
    "gold_eval_md": null
  },
  "artifacts": {
    "diagnostics_md": "reports/preference_diagnostics_20260212_200001001.md",
    "diagnostics_json": "reports/preference_diagnostics_20260212_200001001.json",
    "contentious_jsonl": "artifacts/preference_contentious_20260212_200001001.jsonl"
  }
}
