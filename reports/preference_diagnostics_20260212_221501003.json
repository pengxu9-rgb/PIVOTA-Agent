{
  "ok": true,
  "run_id": "20260212_221501003",
  "final_verdict": "NEED_ADJUDICATION",
  "executive": {
    "overall": {
      "slice_key": "overall",
      "n_votes": 20,
      "sample_count": 20,
      "baseline_votes": 7,
      "variant1_votes": 3,
      "tie_votes": 5,
      "cannot_tell_votes": 5,
      "head_to_head_votes": 10,
      "baseline_win_rate": 0.7,
      "variant1_win_rate": 0.3,
      "tie_rate": 0.25,
      "cannot_tell_rate": 0.25,
      "disagreement_rate": 0.65,
      "baseline_wilson_low": 0.397,
      "baseline_wilson_high": 0.892,
      "variant1_wilson_low": 0.108,
      "variant1_wilson_high": 0.603,
      "_sample_ids": [
        "2dd66d3a38cb93d44231",
        "5312a4f985e1d37718d0",
        "bc6eac20bbd9646293a6",
        "bd40dc07041cc3fc37cf",
        "c69e3f01fb22d037b89e",
        "c9f01cdb0edc5047d390",
        "celebamaskhq_091d02e9e84ef6afa1444be1",
        "celebamaskhq_3c61062163b7c86f48629912",
        "celebamaskhq_545c2c29a045da8358d0d30b",
        "celebamaskhq_5654ed29ec21240d9e02b9d7",
        "celebamaskhq_5ce3df29bd183e29b7a4ac5e",
        "celebamaskhq_68d5cc9d7e8b27307d8e546a",
        "celebamaskhq_b6395251d09e9d886fe6dd5a",
        "lapa_1c1b88eac93e8b27985f5616",
        "lapa_2bd77b8c8eeb4569b6bf296f",
        "lapa_3dec769b833ec7051b102159",
        "lapa_47b96f99409e5ff6e99b65ac",
        "lapa_a3b498b64e0d8909cb4c7887",
        "lapa_aaca1ed5e0aa6f76bbebeb8b",
        "lapa_e6ed8fc0d32e9b790395fd4c"
      ]
    },
    "iaa": {
      "overall_simple_agreement": null,
      "overall_kappa": 0,
      "overlap_samples_total": 6,
      "overlap_samples_labeled_by_2plus": 0,
      "sufficient": false
    }
  },
  "overlay_consistency_gate": {
    "pass": true,
    "thresholds": {
      "coverage_min": 0.98,
      "consistency_min": 0.98,
      "epsilon": 0.000001
    },
    "counts": {
      "eval_rows_total": 20,
      "overlay_present_rows": 20,
      "consistent_rows": 20,
      "joinable_rows": 20,
      "issue_rows": 0
    },
    "rates": {
      "coverage_rate": 1,
      "consistency_rate": 1,
      "joinable_consistency_rate": 1,
      "missing_rate": 0
    },
    "top_issues": []
  },
  "slices": {
    "by_source": [
      {
        "slice_key": "celebamaskhq",
        "n_votes": 7,
        "sample_count": 7,
        "baseline_votes": 1,
        "variant1_votes": 1,
        "tie_votes": 3,
        "cannot_tell_votes": 2,
        "head_to_head_votes": 2,
        "baseline_win_rate": 0.5,
        "variant1_win_rate": 0.5,
        "tie_rate": 0.429,
        "cannot_tell_rate": 0.286,
        "disagreement_rate": 0.571,
        "baseline_wilson_low": 0.095,
        "baseline_wilson_high": 0.905,
        "variant1_wilson_low": 0.095,
        "variant1_wilson_high": 0.905
      },
      {
        "slice_key": "lapa",
        "n_votes": 7,
        "sample_count": 7,
        "baseline_votes": 3,
        "variant1_votes": 2,
        "tie_votes": 1,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 5,
        "baseline_win_rate": 0.6,
        "variant1_win_rate": 0.4,
        "tie_rate": 0.143,
        "cannot_tell_rate": 0.143,
        "disagreement_rate": 0.571,
        "baseline_wilson_low": 0.231,
        "baseline_wilson_high": 0.882,
        "variant1_wilson_low": 0.118,
        "variant1_wilson_high": 0.769
      },
      {
        "slice_key": "internal",
        "n_votes": 6,
        "sample_count": 6,
        "baseline_votes": 3,
        "variant1_votes": 0,
        "tie_votes": 1,
        "cannot_tell_votes": 2,
        "head_to_head_votes": 3,
        "baseline_win_rate": 1,
        "variant1_win_rate": 0,
        "tie_rate": 0.167,
        "cannot_tell_rate": 0.333,
        "disagreement_rate": 0.5,
        "baseline_wilson_low": 0.438,
        "baseline_wilson_high": 1,
        "variant1_wilson_low": 0,
        "variant1_wilson_high": 0.562
      }
    ],
    "by_module": [
      {
        "slice_key": "chin",
        "n_votes": 20,
        "sample_count": 20,
        "baseline_votes": 12,
        "variant1_votes": 8,
        "tie_votes": 0,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 20,
        "baseline_win_rate": 0.6,
        "variant1_win_rate": 0.4,
        "tie_rate": 0,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.4,
        "baseline_wilson_low": 0.387,
        "baseline_wilson_high": 0.781,
        "variant1_wilson_low": 0.219,
        "variant1_wilson_high": 0.613
      },
      {
        "slice_key": "forehead",
        "n_votes": 20,
        "sample_count": 20,
        "baseline_votes": 11,
        "variant1_votes": 9,
        "tie_votes": 0,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 20,
        "baseline_win_rate": 0.55,
        "variant1_win_rate": 0.45,
        "tie_rate": 0,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.45,
        "baseline_wilson_low": 0.342,
        "baseline_wilson_high": 0.742,
        "variant1_wilson_low": 0.258,
        "variant1_wilson_high": 0.658
      },
      {
        "slice_key": "left_cheek",
        "n_votes": 20,
        "sample_count": 20,
        "baseline_votes": 0,
        "variant1_votes": 0,
        "tie_votes": 20,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 0,
        "baseline_win_rate": null,
        "variant1_win_rate": null,
        "tie_rate": 1,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0,
        "baseline_wilson_low": null,
        "baseline_wilson_high": null,
        "variant1_wilson_low": null,
        "variant1_wilson_high": null
      },
      {
        "slice_key": "nose",
        "n_votes": 20,
        "sample_count": 20,
        "baseline_votes": 12,
        "variant1_votes": 8,
        "tie_votes": 0,
        "cannot_tell_votes": 0,
        "head_to_head_votes": 20,
        "baseline_win_rate": 0.6,
        "variant1_win_rate": 0.4,
        "tie_rate": 0,
        "cannot_tell_rate": 0,
        "disagreement_rate": 0.4,
        "baseline_wilson_low": 0.387,
        "baseline_wilson_high": 0.781,
        "variant1_wilson_low": 0.219,
        "variant1_wilson_high": 0.613
      },
      {
        "slice_key": "right_cheek",
        "n_votes": 20,
        "sample_count": 20,
        "baseline_votes": 9,
        "variant1_votes": 6,
        "tie_votes": 0,
        "cannot_tell_votes": 5,
        "head_to_head_votes": 15,
        "baseline_win_rate": 0.6,
        "variant1_win_rate": 0.4,
        "tie_rate": 0,
        "cannot_tell_rate": 0.25,
        "disagreement_rate": 0.55,
        "baseline_wilson_low": 0.357,
        "baseline_wilson_high": 0.802,
        "variant1_wilson_low": 0.198,
        "variant1_wilson_high": 0.643
      }
    ],
    "by_risk_hair": [
      {
        "slice_key": "low(<0.10)",
        "n_votes": 20,
        "sample_count": 20,
        "baseline_votes": 7,
        "variant1_votes": 3,
        "tie_votes": 5,
        "cannot_tell_votes": 5,
        "head_to_head_votes": 10,
        "baseline_win_rate": 0.7,
        "variant1_win_rate": 0.3,
        "tie_rate": 0.25,
        "cannot_tell_rate": 0.25,
        "disagreement_rate": 0.65,
        "baseline_wilson_low": 0.397,
        "baseline_wilson_high": 0.892,
        "variant1_wilson_low": 0.108,
        "variant1_wilson_high": 0.603
      }
    ],
    "by_risk_leakage": [
      {
        "slice_key": "low(<0.05)",
        "n_votes": 20,
        "sample_count": 20,
        "baseline_votes": 7,
        "variant1_votes": 3,
        "tie_votes": 5,
        "cannot_tell_votes": 5,
        "head_to_head_votes": 10,
        "baseline_win_rate": 0.7,
        "variant1_win_rate": 0.3,
        "tie_rate": 0.25,
        "cannot_tell_rate": 0.25,
        "disagreement_rate": 0.65,
        "baseline_wilson_low": 0.397,
        "baseline_wilson_high": 0.892,
        "variant1_wilson_low": 0.108,
        "variant1_wilson_high": 0.603
      }
    ],
    "by_risk_min_pixels": [
      {
        "slice_key": "tiny(<=16)",
        "n_votes": 20,
        "sample_count": 20,
        "baseline_votes": 7,
        "variant1_votes": 3,
        "tie_votes": 5,
        "cannot_tell_votes": 5,
        "head_to_head_votes": 10,
        "baseline_win_rate": 0.7,
        "variant1_win_rate": 0.3,
        "tie_rate": 0.25,
        "cannot_tell_rate": 0.25,
        "disagreement_rate": 0.65,
        "baseline_wilson_low": 0.397,
        "baseline_wilson_high": 0.892,
        "variant1_wilson_low": 0.108,
        "variant1_wilson_high": 0.603
      }
    ],
    "by_guard": [
      {
        "slice_key": "no",
        "n_votes": 20,
        "sample_count": 20,
        "baseline_votes": 7,
        "variant1_votes": 3,
        "tie_votes": 5,
        "cannot_tell_votes": 5,
        "head_to_head_votes": 10,
        "baseline_win_rate": 0.7,
        "variant1_win_rate": 0.3,
        "tie_rate": 0.25,
        "cannot_tell_rate": 0.25,
        "disagreement_rate": 0.65,
        "baseline_wilson_low": 0.397,
        "baseline_wilson_high": 0.892,
        "variant1_wilson_low": 0.108,
        "variant1_wilson_high": 0.603
      }
    ],
    "by_overlay_diff": [
      {
        "slice_key": "high(>=0.03)",
        "n_votes": 20,
        "sample_count": 20,
        "baseline_votes": 7,
        "variant1_votes": 3,
        "tie_votes": 5,
        "cannot_tell_votes": 5,
        "head_to_head_votes": 10,
        "baseline_win_rate": 0.7,
        "variant1_win_rate": 0.3,
        "tie_rate": 0.25,
        "cannot_tell_rate": 0.25,
        "disagreement_rate": 0.65,
        "baseline_wilson_low": 0.397,
        "baseline_wilson_high": 0.892,
        "variant1_wilson_low": 0.108,
        "variant1_wilson_high": 0.603
      }
    ],
    "by_confidence": [
      {
        "slice_key": "high(>=4)",
        "n_votes": 12,
        "sample_count": 12,
        "baseline_votes": 4,
        "variant1_votes": 2,
        "tie_votes": 3,
        "cannot_tell_votes": 3,
        "head_to_head_votes": 6,
        "baseline_win_rate": 0.667,
        "variant1_win_rate": 0.333,
        "tie_rate": 0.25,
        "cannot_tell_rate": 0.25,
        "disagreement_rate": 0.667,
        "baseline_wilson_low": 0.3,
        "baseline_wilson_high": 0.903,
        "variant1_wilson_low": 0.097,
        "variant1_wilson_high": 0.7
      },
      {
        "slice_key": "low(<=2)",
        "n_votes": 4,
        "sample_count": 4,
        "baseline_votes": 1,
        "variant1_votes": 1,
        "tie_votes": 1,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 2,
        "baseline_win_rate": 0.5,
        "variant1_win_rate": 0.5,
        "tie_rate": 0.25,
        "cannot_tell_rate": 0.25,
        "disagreement_rate": 0.75,
        "baseline_wilson_low": 0.095,
        "baseline_wilson_high": 0.905,
        "variant1_wilson_low": 0.095,
        "variant1_wilson_high": 0.905
      },
      {
        "slice_key": "mid(3)",
        "n_votes": 4,
        "sample_count": 4,
        "baseline_votes": 2,
        "variant1_votes": 0,
        "tie_votes": 1,
        "cannot_tell_votes": 1,
        "head_to_head_votes": 2,
        "baseline_win_rate": 1,
        "variant1_win_rate": 0,
        "tie_rate": 0.25,
        "cannot_tell_rate": 0.25,
        "disagreement_rate": 0.5,
        "baseline_wilson_low": 0.342,
        "baseline_wilson_high": 1,
        "variant1_wilson_low": 0,
        "variant1_wilson_high": 0.658
      }
    ]
  },
  "disagreement_drivers": [
    {
      "slice_type": "confidence",
      "slice_key": "low(<=2)",
      "n_votes": 4,
      "cannot_tell_rate": 0.25,
      "disagreement_rate": 0.75,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0.281,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.413
    },
    {
      "slice_type": "confidence",
      "slice_key": "high(>=4)",
      "n_votes": 12,
      "cannot_tell_rate": 0.25,
      "disagreement_rate": 0.667,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0.275,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.379
    },
    {
      "slice_type": "guard_triggered",
      "slice_key": "no",
      "n_votes": 20,
      "cannot_tell_rate": 0.25,
      "disagreement_rate": 0.65,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0.276,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.373
    },
    {
      "slice_type": "hair_bucket",
      "slice_key": "low(<0.10)",
      "n_votes": 20,
      "cannot_tell_rate": 0.25,
      "disagreement_rate": 0.65,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0.276,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.373
    },
    {
      "slice_type": "leakage_bucket",
      "slice_key": "low(<0.05)",
      "n_votes": 20,
      "cannot_tell_rate": 0.25,
      "disagreement_rate": 0.65,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0.276,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.373
    },
    {
      "slice_type": "min_pixels_bucket",
      "slice_key": "tiny(<=16)",
      "n_votes": 20,
      "cannot_tell_rate": 0.25,
      "disagreement_rate": 0.65,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0.276,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.373
    },
    {
      "slice_type": "overlay_diff_bucket",
      "slice_key": "high(>=0.03)",
      "n_votes": 20,
      "cannot_tell_rate": 0.25,
      "disagreement_rate": 0.65,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0.276,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.373
    },
    {
      "slice_type": "source",
      "slice_key": "celebamaskhq",
      "n_votes": 7,
      "cannot_tell_rate": 0.286,
      "disagreement_rate": 0.571,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0.273,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.357
    },
    {
      "slice_type": "source",
      "slice_key": "internal",
      "n_votes": 6,
      "cannot_tell_rate": 0.333,
      "disagreement_rate": 0.5,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0.276,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.35
    },
    {
      "slice_type": "module",
      "slice_key": "right_cheek",
      "n_votes": 20,
      "cannot_tell_rate": 0.25,
      "disagreement_rate": 0.55,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0.276,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.333
    },
    {
      "slice_type": "confidence",
      "slice_key": "mid(3)",
      "n_votes": 4,
      "cannot_tell_rate": 0.25,
      "disagreement_rate": 0.5,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0.275,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.313
    },
    {
      "slice_type": "source",
      "slice_key": "lapa",
      "n_votes": 7,
      "cannot_tell_rate": 0.143,
      "disagreement_rate": 0.571,
      "avg_split_close": 0,
      "avg_hair_overlap": 0,
      "avg_leakage_bg": 0,
      "avg_min_module_pixels": 0,
      "avg_overlay_diff_ratio": 0.28,
      "likely_cause": "model outputs unstable",
      "issue_area": "model issue",
      "score": 0.293
    }
  ],
  "actions": [
    {
      "rank": 1,
      "title": "Increase visual separability in A/B overlays",
      "what_to_change": "Update `scripts/preference_round1_real_runbook.mjs` overlay rendering to add contour-diff inset and run a focused sweep with `PREFERENCE_MAX_EDGE=768`.",
      "target_slice": "confidence:low(<=2)",
      "why": "cannot_tell_rate=0.25 with avg_split_close=0 indicates small visible deltas.",
      "validate": "Run `make preference-round1-real-pack ... TARGET_TOTAL=80 PREFERENCE_MAX_EDGE=768` then `make preference-final ...`; expect cannot_tell_rate to drop by >=0.05 without lowering IAA."
    },
    {
      "rank": 2,
      "title": "Tighten labeling rubric for ambiguous modules",
      "what_to_change": "Refine `label_studio/project_preference_ab.xml` instructions and `docs/GOLD_LABELING_GUIDE.md` for tie/cannot_tell usage on under-eye and low-detail regions.",
      "target_slice": "module:under_eye_*",
      "why": "under-eye slice cannot_tell_rate=- and disagreement_rate=-.",
      "validate": "Re-run overlap subset (>=40) and check IAA improves (kappa +0.05) while cannot_tell on under-eye decreases."
    },
    {
      "rank": 3,
      "title": "Harden forehead hair/skin boundary behavior",
      "what_to_change": "Keep hair-aware forehead clip path and prioritize forehead/hair hard cases for skinmask+hair-mask retraining decision; tune oval clip params in offline AB (`DIAG_FACE_OVAL_CLIP_MIN_PIXELS`, `DIAG_FACE_OVAL_CLIP_MIN_KEEP_RATIO`).",
      "target_slice": "forehead + high hair_overlap_est",
      "why": "forehead variant win=0.45, baseline win=0.55; disagreement driver highlights boundary instability.",
      "validate": "Run `make eval-gold-ab ...` + preference rerun on hair-high subset; expect forehead variant win +>=0.10 and lower disagreement in high-hair bucket."
    },
    {
      "rank": 4,
      "title": "Stabilize hard-case guard strategy",
      "what_to_change": "Tune guard behavior on hard samples via offline variant sweep (variant2 under-eye guard relaxation + fallback behavior) and compare only guard-triggered subset.",
      "target_slice": "confidence:low(<=2)",
      "why": "high disagreement slice score=0.413 suggests unstable model outputs on hard cases.",
      "validate": "Run a guard-triggered mini-pack (`TARGET_TOTAL=80`, stress-heavy) and expect disagreement_rate drop by >=0.08 with no increase in cannot_tell."
    },
    {
      "rank": 5,
      "title": "Address internal style mismatch and pipeline artifacts",
      "what_to_change": "If internal remains worse, add internal-style slices to training/eval packs and inspect crop pipeline (`input_thumb` generation) for over-aggressive resizing.",
      "target_slice": "source:internal",
      "why": "internal disagreement_rate=0.5 vs overall=0.65; crossset leakage=-.",
      "validate": "Run internal-only preference round (`LIMIT_INTERNAL` up, external down) and confirm internal cannot_tell/disagreement converge toward external slices."
    }
  ],
  "contentious_export": {
    "path": "artifacts/preference_contentious_20260212_221501003.jsonl",
    "samples": 20
  },
  "optional_context": {
    "crossset": null,
    "gold_eval": null
  },
  "proposer_summary": {
    "suggested_overlay_diff_filter_min": 0.01,
    "very_low_overlay_diff_vote_rate": 0,
    "guidance": "prioritize contentious samples with overlay_diff_ratio>=0.01 and downweight overlay_diff_ratio<0.01"
  },
  "generated_at": "2026-02-12T23:36:57.460Z",
  "inputs": {
    "manifest": "artifacts/preference_round1_20260212_221501003/manifest.json",
    "eval": "reports/eval_preference_20260212_221501003.jsonl",
    "labels": "artifacts/preference_round1_20260212_221501003/preference_labels.ndjson",
    "crossset": null,
    "gold_eval_md": null
  },
  "artifacts": {
    "diagnostics_md": "reports/preference_diagnostics_20260212_221501003.md",
    "diagnostics_json": "reports/preference_diagnostics_20260212_221501003.json",
    "contentious_jsonl": "artifacts/preference_contentious_20260212_221501003.jsonl"
  }
}
